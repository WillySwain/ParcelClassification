{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f79f28d",
   "metadata": {},
   "source": [
    "# Gather Dataset\n",
    "The dataset will be gathered using Google's iCrawler, all code below by William"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e546188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icrawler.builtin import GoogleImageCrawler\n",
    "import datetime\n",
    "def datetime2tuple(date):\n",
    "    return (date.year, date.month, date.day)\n",
    "def gather_data(n_total_images,n_per_crawl,keyword, folder_name): #collects data by sending google different search queries with different date constraints to mitigate duplicate images\n",
    "    delta = datetime.timedelta(days=30)\n",
    "    end_day = datetime.datetime(2023, 3, 17)\n",
    "    for i in range(int(n_total_images / n_per_crawl )):\n",
    "        start_day = end_day - delta\n",
    "        google_crawler = GoogleImageCrawler(downloader_threads=4, storage={'root_dir': folder_name})\n",
    "        google_crawler.crawl(keyword=keyword[i], filters={'date':(datetime2tuple(start_day), datetime2tuple(end_day))}, file_idx_offset=i*n_per_crawl , max_num=n_per_crawl)\n",
    "        end_day = start_day - datetime.timedelta(days=1)\n",
    "\n",
    "#each call gathers data from the query and sends it to its respective directory to be manually cleaned\n",
    "\n",
    "#gather_data(900,100,['cardboard boxes with tape on it','cardboard boxes with tape on it','packages ready to ship','packages about to be sent','cardboard boxes',\n",
    "#           'packages','box packages','sealed packages','delivery cardboard package box'],'data/startingdata/goodpackages')\n",
    "#gather_data(200,100,['ripped cardboard boxes','ripped packages'],'data/startingdata/rippedpackages') #ripped\n",
    "#gather_data(200,100,['destroyed packages','damaged cardboard boxes'],'data/startingdata/crushedpackages') #crushed\n",
    "#gather_data(200,100,['cardboard boxes with water damage','wet cardboard boxes'],'data/startingdata/leakingpackages')#leaking\n",
    "#gather_data(200,100,['burned cardboard','charred cardboard'],'data/startingdata/burnedpackages') # burned\n",
    "#gather_data(200,100,['cardboard recycling pile','broken down cardboard boxes'],'data/startingdata/foldedpackages')#folded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c919911c",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "225642ca",
   "metadata": {},
   "source": [
    "## Split the Data\n",
    "After manually cleaning the data, it is ready to be split into testing and training sets.\n",
    "The different categories of data will be merged into \"bad packages\" to allow for binary image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becb3e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1000 files [00:01, 503.83 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "import shutil\n",
    "import os\n",
    "input_folder = \"data\\\\startingdata\\\\\"\n",
    "output_folder = \"data\\\\split data\\\\\"\n",
    "splitfolders.ratio(input_folder, output=output_folder, #splits the data 80% for training and 20% for testing\n",
    "                   seed=2, ratio=(.6, .2, .2))\n",
    "subsets = ['test\\\\', 'train\\\\','val\\\\'] \n",
    "for subset in subsets: #will be moving images for the training and testing sets\n",
    "    output = output_folder+subset\n",
    "    destination = \"data\\\\split data\\\\\"+subset+\"\\\\badpackages\\\\\"\n",
    "    categories = ['burnedpackages', 'crushedpackages',\n",
    "                  'foldedpackages', 'leakingpackages', 'rippedpackages']\n",
    "    for category in categories: #will merge each directory of bad packages into a single one\n",
    "        source_folder = output + category\n",
    "        for file_name in os.listdir(source_folder):\n",
    "\n",
    "            if os.path.exists(destination + '\\\\' + file_name): #if the already path exists, it renames the new image and moves it\n",
    "                data = os.path.splitext(file_name)\n",
    "                only_name = data[0]\n",
    "                extension = data[1]\n",
    "                new_base = category + only_name + extension\n",
    "                new_name = os.path.join(destination, new_base)\n",
    "                shutil.move(source_folder + '\\\\' + file_name, new_name)\n",
    "            else:\n",
    "                shutil.move(source_folder + '\\\\' + file_name, #if the path is not already used, the new file will take its place\n",
    "                            destination + '\\\\' + file_name)\n",
    "        os.rmdir(source_folder)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f98331f4",
   "metadata": {},
   "source": [
    "## Processing the Data\n",
    "The data will be preprocessed using Keras' ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bbcabae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import preprocess_input,ResNet50\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "img_height,img_width = (224,224) #the size required by resnet\n",
    "\n",
    "train_data_path = \"data/split data/train/\"\n",
    "test_data_path = \"data/split data/test/\"\n",
    "val_data_path = \"data/split data/val/\"\n",
    "training_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,horizontal_flip=True,validation_split=0.4) #randomly flips images and saves 40% for validation\n",
    "training_data_generator = training_datagen.flow_from_directory(train_data_path,target_size=(img_height,img_width),class_mode='categorical',subset='training')\n",
    "testing_data_generator = training_datagen.flow_from_directory(test_data_path,target_size=(img_height,img_width),batch_size=1,class_mode='categorical',subset='validation')\n",
    "validation_data_generator = training_datagen.flow_from_directory(val_data_path,target_size=(img_height,img_width),batch_size=1,class_mode='categorical',subset='validation')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4df0abdb",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d779234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 8/12 [===================>..........] - ETA: 5s - loss: 1.4657"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 34s 2s/step - loss: 1.0930 - val_loss: 0.4650\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 28s 2s/step - loss: 0.3079 - val_loss: 0.4650\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.1395 - val_loss: 0.4329\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 28s 2s/step - loss: 0.0770 - val_loss: 0.4833\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.0567 - val_loss: 0.4277\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 28s 2s/step - loss: 0.0347 - val_loss: 0.4583\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 28s 2s/step - loss: 0.0223 - val_loss: 0.4550\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 28s 2s/step - loss: 0.0149 - val_loss: 0.5704\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.0175 - val_loss: 0.5312\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 28s 2s/step - loss: 0.0115 - val_loss: 0.5227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eea2c0dbb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True #This is required to get around an error thrown by larger images\n",
    "\n",
    "base_model = ResNet50(include_top=False,weights='imagenet')\n",
    "\n",
    "output=base_model.output #add new layers to the model\n",
    "output=GlobalAveragePooling2D()(output) #adds global average poolinglayer\n",
    "output=Dense(1024,activation='relu')(output)  #adds dense layer\n",
    "predictions = Dense(training_data_generator.num_classes,activation='softmax')(output)\n",
    "model = Model(inputs=base_model.input,outputs=predictions) \n",
    "\n",
    "for layer in base_model.layers: #freezes all of the weights in the base_model layers\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy') \n",
    "model.fit(training_data_generator,epochs=10,validation_data = validation_data_generator) # trains the newly added layers [] learning rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "644fba3b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3e16e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    }
   ],
   "source": [
    "samples = len(testing_data_generator)\n",
    "predicted = []\n",
    "actual=[]\n",
    "#print(testing_data_generator.class_indices)\n",
    "for i in range(samples): #runs each sample in the test set\n",
    "    x,y=testing_data_generator.next() #x is the image and y is the label\n",
    "    predicted.append((model.predict(x)))\n",
    "    actual.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5bc696a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.875\n",
      "Precision: 0.875\n",
      "Recall: 0.875\n",
      "Specificity: 0.875\n",
      "F1-Score: 0.875\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#model.save('Models\\\\Resnet50\\\\Resnet50_with_50_10_datasplit.h5')\n",
    "predicted_class = [i.argmax() for i in predicted]\n",
    "actual_class = [i.argmax() for i in actual]\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_class, predicted_class) #creates confusion matrix from predictions and actuals\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predicted label', fontsize=18)\n",
    "plt.ylabel('Actual label', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "precision = (conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[0][1])\n",
    "recall = (conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[1][0])\n",
    "\n",
    "print(\"Accuracy:\",  (conf_matrix[0][0]+conf_matrix[1][1])/((conf_matrix[0][0]+conf_matrix[0][1])+(conf_matrix[1][0]+conf_matrix[1][1])))\n",
    "print(\"Precision:\",  precision)\n",
    "print(\"Recall:\",  recall)\n",
    "print(\"Specificity:\",  (conf_matrix[1][1])/(conf_matrix[1][1]+conf_matrix[0][1]))\n",
    "print(\"F1-Score:\",  (2*precision * recall)/(precision+recall))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dd5c455",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abccfc6e",
   "metadata": {},
   "source": [
    "## Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133c07c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n",
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input,VGG19\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "img_height,img_width = (224,224) #the size required by resnet\n",
    "\n",
    "train_data_path = \"data/split data/train/\"\n",
    "test_data_path = \"data/split data/test/\"\n",
    "val_data_path = \"data/split data/val/\"\n",
    "\n",
    "VGG_training_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,horizontal_flip=True,validation_split=0.4) #randomly flips images and saves 40% for validation\n",
    "VGG_training_data_generator = VGG_training_datagen.flow_from_directory(train_data_path,target_size=(img_height,img_width),class_mode='categorical',subset='training')\n",
    "VGG_testing_data_generator = VGG_training_datagen.flow_from_directory(test_data_path,target_size=(img_height,img_width),batch_size=1,class_mode='categorical',subset='validation')\n",
    "VGG_validation_data_generator = VGG_training_datagen.flow_from_directory(val_data_path,target_size=(img_height,img_width),batch_size=1,class_mode='categorical',subset='validation')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed0ef457",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f8bf469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - ETA: 0s - loss: 1.0683"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True #This is required to get around an error thrown by larger images\n",
    "\n",
    "base_model = VGG19(include_top=False,weights='imagenet')\n",
    "\n",
    "output=base_model.output #add new layers to the model\n",
    "output=GlobalAveragePooling2D()(output) #adds global average poolinglayer\n",
    "output=Dense(1024,activation='relu')(output)  #adds dense layer\n",
    "predictions = Dense(VGG_training_data_generator.num_classes,activation='softmax')(output)\n",
    "model = Model(inputs=base_model.input,outputs=predictions) \n",
    "\n",
    "for layer in base_model.layers: #freezes all of the weights in the base_model layers\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy') \n",
    "model.fit(VGG_training_data_generator,epochs=10,validation_data = VGG_validation_data_generator) # trains the newly added layers\n",
    "model.save('Models\\\\VGG.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f50305a9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d614910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 315ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n"
     ]
    }
   ],
   "source": [
    "samples = len(VGG_testing_data_generator)\n",
    "predicted = []\n",
    "actual=[]\n",
    "#print(testing_data_generator.class_indices)\n",
    "for i in range(samples): #runs each sample in the test set\n",
    "    x,y=VGG_testing_data_generator.next() #x is the image and y is the label\n",
    "    predicted.append((model.predict(x)))\n",
    "    actual.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43258bf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\willi\\PycharmProjects\\ParcelClassification\\ParcelClassification.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/willi/PycharmProjects/ParcelClassification/ParcelClassification.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/willi/PycharmProjects/ParcelClassification/ParcelClassification.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/willi/PycharmProjects/ParcelClassification/ParcelClassification.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m predicted_class \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39margmax() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m predicted]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/willi/PycharmProjects/ParcelClassification/ParcelClassification.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m actual_class \u001b[39m=\u001b[39m [i\u001b[39m.\u001b[39margmax() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m actual]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/willi/PycharmProjects/ParcelClassification/ParcelClassification.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m conf_matrix \u001b[39m=\u001b[39m confusion_matrix(actual_class, predicted_class) \u001b[39m#creates confusion matrix from predictions and actuals\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted_class = [i.argmax() for i in predicted]\n",
    "actual_class = [i.argmax() for i in actual]\n",
    "\n",
    "conf_matrix = confusion_matrix(actual_class, predicted_class) #creates confusion matrix from predictions and actuals\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predicted label', fontsize=18)\n",
    "plt.ylabel('Actual label', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "precision = (conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[0][1])\n",
    "recall = (conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[1][0])\n",
    "\n",
    "print(\"Accuracy:\",  (conf_matrix[0][0]+conf_matrix[1][1])/((conf_matrix[0][0]+conf_matrix[0][1])+(conf_matrix[1][0]+conf_matrix[1][1])))\n",
    "print(\"Precision:\",  precision)\n",
    "print(\"Recall:\",  recall)\n",
    "print(\"Specificity:\",  (conf_matrix[1][1])/(conf_matrix[1][1]+conf_matrix[0][1]))\n",
    "print(\"F1-Score:\",  (2*precision * recall)/(precision+recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb291d6f",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "start_folder = 'data/split data/'\n",
    "category = [\"test/\",\"train/\"]\n",
    "category = [\"test/\",\"train/\",\"val/\"]\n",
    "classification = [\"goodpackages\",\"badpackages\"]\n",
    "for type in category:\n",
    "    class_folder = start_folder + type\n",
    "    for class_type in classification:\n",
    "        folder = class_folder + class_type\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348d454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1870f1194fb5b3d43b6d32e845741389586dbe9c4e1e45e17e0f6602cfe22778"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

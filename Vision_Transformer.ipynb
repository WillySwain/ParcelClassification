{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MCTZFkFw6i6",
        "outputId": "2314a1f3-33c6-45a1-c7f1-bdef29fdbff4"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC9HqG5u750_",
        "outputId": "336719f6-2a67-48f2-c7b4-1f0762380c0b"
      },
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"AvkRcIYAvUA2WtYkRunR\")\n",
        "project = rf.workspace().project(\"parcel-damage-classification\")\n",
        "dataset = project.version(2).download(\"folder\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.9/dist-packages (1.0.3)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.9/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from roboflow) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.7.0.72)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.9/dist-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.26.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.27.1)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.9/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.9/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from roboflow) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (1.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->roboflow) (4.39.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->roboflow) (2.0.12)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->roboflow) (3.15.0)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Parcel-Damage-Classification-2 to folder: 100% [8417952 / 8417952] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Parcel-Damage-Classification-2 in folder:: 100%|██████████| 1011/1011 [00:00<00:00, 3758.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgn-a3NiSjqR"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_ds = torchvision.datasets.ImageFolder(dataset.location + '/train/', transform=ToTensor())\n",
        "valid_ds = torchvision.datasets.ImageFolder(dataset.location + '/valid/', transform=ToTensor())\n",
        "test_ds = torchvision.datasets.ImageFolder(dataset.location + '/test/', transform=ToTensor())"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyHJjDYLfoFy"
      },
      "source": [
        "## Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGDrb1Q4ToLN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14324754-5797-4956-88c3-d6ca6b1bc5ca"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import ViTModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ViTForImageClassification(nn.Module):\n",
        "    def __init__(self, num_labels=3):\n",
        "        super(ViTForImageClassification, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def forward(self, pixel_values, labels):\n",
        "        outputs = self.vit(pixel_values=pixel_values)\n",
        "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          loss_fct = nn.CrossEntropyLoss()\n",
        "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        if loss is not None:\n",
        "          return logits, loss.item()\n",
        "        else:\n",
        "          return logits, None"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.0.dev0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BePiLK-LtXuG"
      },
      "source": [
        "## Define the Model Parameters\n",
        "\n",
        "To train this model, we will train in 3 epochs, with a batch size of 10 and a learning rate of 2e-5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntGvS0_wUAxc"
      },
      "source": [
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 2e-5"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At9H-QOStt8_"
      },
      "source": [
        "We will use the pretrained Vision Transformer feature extractor, an Adam Optimizer, and a Cross Entropy Loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIyJr8EDtvlR",
        "outputId": "57b11e98-3fae-4f87-d823-e509fb1d63e6"
      },
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "# Define Model\n",
        "model = ViTForImageClassification(len(train_ds.classes))    \n",
        "# Feature Extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "# Adam Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "# Cross Entropy Loss\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# Use GPU if available  \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "if torch.cuda.is_available():\n",
        "    model.cuda() "
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTZ-BP1yu1Us"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ql2T5PDUI1D",
        "outputId": "68f76cc0-7068-44cd-f844-4db3234febb7"
      },
      "source": [
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "\n",
        "print(\"Number of train samples: \", len(train_ds))\n",
        "print(\"Number of test samples: \", len(test_ds))\n",
        "print(\"Detected Classes are: \", train_ds.class_to_idx) \n",
        "\n",
        "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
        "test_loader  = data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) \n",
        "\n",
        "# Train the model\n",
        "for epoch in range(EPOCHS):        \n",
        "  for step, (x, y) in enumerate(train_loader):\n",
        "    # Change input array into list with each batch being one element\n",
        "    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
        "    # Remove unecessary dimension\n",
        "    for index, array in enumerate(x):\n",
        "      x[index] = np.squeeze(array)\n",
        "    # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
        "    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
        "    # Send to GPU if available\n",
        "    x, y  = x.to(device), y.to(device)\n",
        "    b_x = Variable(x)   # batch x (image)\n",
        "    b_y = Variable(y)   # batch y (target)\n",
        "    # Feed through model\n",
        "    output, loss = model(b_x, None)\n",
        "    # Calculate loss\n",
        "    if loss is None: \n",
        "      loss = loss_func(output, b_y)   \n",
        "      optimizer.zero_grad()           \n",
        "      loss.backward()                 \n",
        "      optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "      # Get the next batch for testing purposes\n",
        "      test = next(iter(test_loader))\n",
        "      test_x = test[0]\n",
        "      # Reshape and get feature matrices as needed\n",
        "      test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
        "      for index, array in enumerate(test_x):\n",
        "        test_x[index] = np.squeeze(array)\n",
        "      test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
        "      # Send to appropirate computing device\n",
        "      test_x = test_x.to(device)\n",
        "      test_y = test[1].to(device)\n",
        "      # Get output (+ respective class) and compare to target\n",
        "      test_output, loss = model(test_x, test_y)\n",
        "      test_output = test_output.argmax(1)\n",
        "      # Calculate Accuracy\n",
        "      accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
        "      print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train samples:  704\n",
            "Number of test samples:  99\n",
            "Detected Classes are:  {'damagedpackages': 0, 'undamagedpackages': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 | train loss: 0.6715 | test accuracy: 0.75\n",
            "Epoch:  0 | train loss: 0.6137 | test accuracy: 0.75\n",
            "Epoch:  0 | train loss: 0.6946 | test accuracy: 0.50\n",
            "Epoch:  0 | train loss: 0.7033 | test accuracy: 0.50\n",
            "Epoch:  1 | train loss: 0.6901 | test accuracy: 0.75\n",
            "Epoch:  1 | train loss: 0.6910 | test accuracy: 0.50\n",
            "Epoch:  1 | train loss: 0.5706 | test accuracy: 1.00\n",
            "Epoch:  1 | train loss: 0.7053 | test accuracy: 0.25\n",
            "Epoch:  2 | train loss: 0.7310 | test accuracy: 0.25\n",
            "Epoch:  2 | train loss: 0.7076 | test accuracy: 0.25\n",
            "Epoch:  2 | train loss: 0.7207 | test accuracy: 0.25\n",
            "Epoch:  2 | train loss: 0.7149 | test accuracy: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWXvWiB-srBC"
      },
      "source": [
        "## Evaluate on a Test Image\n",
        "\n",
        "Finally, let's evaluate the model on a test image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLv_xdYssuGO",
        "outputId": "b29972b9-d4f2-4d22-afc0-3e1a679d8ab6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "EVAL_BATCH = 1\n",
        "eval_loader  = data.DataLoader(test_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4) \n",
        "predicted_class = []\n",
        "actual_class = []\n",
        "\n",
        "for inputs, target in iter(eval_loader):\n",
        "\n",
        "  # Disable grad\n",
        "  with torch.no_grad():\n",
        "      \n",
        "\n",
        "    # Reshape and get feature matrices as needed\n",
        "    print(inputs.shape)\n",
        "    inputs = inputs[0].permute(1, 2, 0)\n",
        "    # Save original Input\n",
        "    originalInput = inputs\n",
        "    for index, array in enumerate(inputs):\n",
        "      inputs[index] = np.squeeze(array)\n",
        "    inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
        "\n",
        "    # Send to appropriate computing device\n",
        "    inputs = inputs.to(device)\n",
        "    target = target.to(device)\n",
        "  \n",
        "    # Generate prediction\n",
        "    prediction, loss = model(inputs, target)\n",
        "      \n",
        "    # Predicted class value using argmax\n",
        "    predicted_classif = np.argmax(prediction.cpu())\n",
        "    value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_classif)]\n",
        "    value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
        "          \n",
        "    actual_class.append(target.detach().cpu().numpy()[0])\n",
        "    predicted_class.append(predicted_classif.detach().cpu().numpy())\n",
        "    #predicted_list.append()\n",
        "    # Show result\n",
        "    # plt.imshow(originalInput)\n",
        "    # plt.xlim(224,0)\n",
        "    # plt.ylim(224,0)\n",
        "    # plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n",
        "    # plt.show()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n",
            "torch.Size([1, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_matrix = confusion_matrix(actual_class, predicted_class) #creates confusion matrix from predictions and actuals\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
        "\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        " \n",
        "plt.xlabel('Predicted label', fontsize=18)\n",
        "plt.ylabel('Actual label', fontsize=18)\n",
        "plt.title('Confusion Matrix', fontsize=18)\n",
        "plt.show()\n",
        "\n",
        "precision = (conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[0][1])\n",
        "recall = (conf_matrix[0][0])/(conf_matrix[0][0]+conf_matrix[1][0])\n",
        "\n",
        "print(\"Accuracy:\",  (conf_matrix[0][0]+conf_matrix[1][1])/((conf_matrix[0][0]+conf_matrix[0][1])+(conf_matrix[1][0]+conf_matrix[1][1])))\n",
        "print(\"Precision:\",  precision)\n",
        "print(\"Recall:\",  recall)\n",
        "print(\"Specificity:\",  (conf_matrix[1][1])/(conf_matrix[1][1]+conf_matrix[0][1]))\n",
        "print(\"F1-Score:\",  (2*precision * recall)/(precision+recall))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "3HbJoqHuKjpK",
        "outputId": "d648ebb9-43b6-4211-c8e1-334003d38220"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 750x750 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.46464646464646464\n",
            "Precision: 0.03636363636363636\n",
            "Recall: 1.0\n",
            "Specificity: 0.4536082474226804\n",
            "F1-Score: 0.07017543859649122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGVIM5jX2Z5b"
      },
      "source": [
        "## Save the Entire Model\n",
        "\n",
        "We can save the entire model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4S-BcSI2v88"
      },
      "source": [
        "torch.save(model, '/content/model.pt')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeJ99BfV9QRo"
      },
      "source": [
        "## Export Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFmfiH0Z3QJb",
        "outputId": "991a3b09-3af2-4c9f-dc60-c5608110a05f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cp /content/model.pt /content/gdrive/My\\ Drive"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORZKpeH1REHP"
      },
      "source": [
        "## Use your Exported Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS706FbORDiO",
        "outputId": "ae6923f2-77a1-4037-c468-8e453ebb0b42"
      },
      "source": [
        "MODEL_PATH = '/content/model.pt'\n",
        "model = torch.load(MODEL_PATH)\n",
        "model.eval()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    }
  ]
}